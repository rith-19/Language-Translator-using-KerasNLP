{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGMBt8Z6uH2F"
      },
      "source": [
        "# **University of Madras,Guindy Campus**#\n",
        "# **Department of Computer Science**\n",
        "**II M.Sc Computer Science**\n",
        "\n",
        "Team-1:\n",
        "\n",
        "Angel Sarah Josephine B\n",
        "\n",
        "Deepika M\n",
        "\n",
        "Rithish R\n",
        "\n",
        "Singavarapu Rohit Roy\n",
        "\n",
        "Sunil Kumar M\n",
        "\n",
        "Syed Aljibre A\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaeV95t6XbaM"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    vocab_size = 15000 # Vocabulary Size\n",
        "    sequence_length = 20\n",
        "    batch_size = 20\n",
        "    validation_split = 0.3\n",
        "    embed_dim = 256\n",
        "    latent_dim = 256\n",
        "    num_heads = 2\n",
        "    epochs = 10 # Number of Epochs to train\n",
        "    start_token = \"[start]\"\n",
        "    end_token = \"[end]\"\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dx-l3eQNZAvE"
      },
      "outputs": [],
      "source": [
        "!pip install keras-nlp --upgrade\n",
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubCdzTLHZ-BI"
      },
      "outputs": [],
      "source": [
        "import keras_nlp\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.layers import TextVectorization\n",
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Out5nuLP0yMl"
      },
      "outputs": [],
      "source": [
        "data = pd.read_excel(\"/content/data - Copy.xlsx\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcOJzQ8N446B"
      },
      "outputs": [],
      "source": [
        "data[\"tamil\"] = data[\"tamil\"].apply(lambda item: f\"{config.start_token} \" + item + f\" {config.end_token}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIbdwo_C04cG"
      },
      "outputs": [],
      "source": [
        "strip_chars = string.punctuation + \"Â¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "def tamil_standardize(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\"%re.escape(strip_chars), \"\")\n",
        "english_vectorization = TextVectorization(\n",
        "    max_tokens=config.vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=config.sequence_length,\n",
        ")\n",
        "tamil_vectorization = TextVectorization(\n",
        "    max_tokens=config.vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=config.sequence_length + 1,\n",
        "    standardize=tamil_standardize,\n",
        ")\n",
        "\n",
        "english_vectorization.adapt(list(data[\"english\"]))\n",
        "tamil_vectorization.adapt(list(data[\"tamil\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYAGhBZo1V5f"
      },
      "outputs": [],
      "source": [
        "def preprocess(english, tamil):\n",
        "    english = english_vectorization(english)\n",
        "    tamil = tamil_vectorization(tamil)\n",
        "    return ({\"encoder_inputs\": english, \"decoder_inputs\": tamil[:, :-1]}, tamil[:, 1:])\n",
        "def make_dataset(df, batch_size, mode):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(df[\"english\"]), list(df[\"tamil\"])))\n",
        "    if mode == \"train\":\n",
        "       dataset = dataset.shuffle(batch_size * 4)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(preprocess)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE).cache()\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5RDnxKZ1ZX0"
      },
      "outputs": [],
      "source": [
        "train, valid = train_test_split(data, test_size=config.validation_split)\n",
        "train.shape, valid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EArTDsnv1blq"
      },
      "outputs": [],
      "source": [
        "train_ds = make_dataset(train, batch_size=config.batch_size, mode=\"train\")\n",
        "valid_ds = make_dataset(valid, batch_size=config.batch_size, mode=\"valid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BD7FKI951g18"
      },
      "outputs": [],
      "source": [
        "def get_model(config):\n",
        "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "    x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "        config.vocab_size,\n",
        "        config.sequence_length,\n",
        "        config.embed_dim,\n",
        "        mask_zero=True\n",
        "    )(encoder_inputs)\n",
        "    encoder_outputs = keras_nlp.layers.TransformerEncoder(intermediate_dim=config.embed_dim, num_heads=config.num_heads)(x)\n",
        "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "    encoded_seq_inputs = keras.Input(shape=(None, config.embed_dim), name=\"decoder_state_inputs\")\n",
        "    x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "        config.vocab_size,\n",
        "        config.sequence_length,\n",
        "        config.embed_dim,\n",
        "        mask_zero=True\n",
        "    )(decoder_inputs)\n",
        "    x = keras_nlp.layers.TransformerDecoder(config.latent_dim, config.num_heads)(x, encoded_seq_inputs)\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    decoder_outputs = layers.Dense(config.vocab_size, activation=\"softmax\")(x)\n",
        "    decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "    transformer = keras.Model(\n",
        "        [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        "    )\n",
        "    transformer.compile(\n",
        "        \"adam\",\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\n",
        "            \"accuracy\"\n",
        "        ]\n",
        "    )\n",
        "    return transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJxHkmxI1lNs"
      },
      "outputs": [],
      "source": [
        "model_ta = get_model(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDb_iwUJ1tgR"
      },
      "outputs": [],
      "source": [
        "checkpoints = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"model_ta.tf\",\n",
        "    monitor=\"val_accuracy\",\n",
        "    mode=\"min\",\n",
        "    save_best_only=True\n",
        ")\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=10,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    restore_best_weights=True\n",
        ")\n",
        "history = model_ta.fit(train_ds, epochs=config.epochs, validation_data=valid_ds, callbacks=[checkpoints, early_stop])\n",
        "accuracy = model_ta.evaluate(train_ds, return_dict=True)['accuracy']\n",
        "print(f'Accuracy Of Tamil: {accuracy*100:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRHxLTNi25jo"
      },
      "outputs": [],
      "source": [
        "loaded_model_ta = tf.keras.models.load_model(\"model_ta.tf\", custom_objects={\n",
        "    \"TokenAndPositionEmbedding\": keras_nlp.layers.TokenAndPositionEmbedding,\n",
        "    \"TransformerEncoder\": keras_nlp.layers.TransformerEncoder,\n",
        "    \"TransformerDecoder\": keras_nlp.layers.TransformerDecoder\n",
        "})\n",
        "loaded_model_ta.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDt0ls0VWvbL"
      },
      "outputs": [],
      "source": [
        "tamil_vocab = tamil_vectorization.get_vocabulary()\n",
        "tamil_index_lookup = dict(zip(range(len(tamil_vocab)), tamil_vocab))\n",
        "start_index = tamil_vocab.index(config.start_token)\n",
        "end_index = tamil_vocab.index(config.end_token)\n",
        "unk_index = tamil_vocab.index(\"[UNK]\")\n",
        "def decode_sequence_ta(model_ta, input_sentence, filtered_values = [start_index, end_index, unk_index]):\n",
        "    tokenized_input_sentence = english_vectorization([input_sentence])\n",
        "    decoded_sentence = [start_index] + [0] * (config.sequence_length)\n",
        "    for i in range(config.sequence_length):\n",
        "        decoded_sentence_constant = tf.constant([decoded_sentence[:config.sequence_length]])\n",
        "        predictions = model_ta([tokenized_input_sentence, decoded_sentence_constant])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        decoded_sentence[i + 1] = sampled_token_index\n",
        "        if sampled_token_index == end_index:\n",
        "            break\n",
        "    components = [tamil_index_lookup[c] for c in decoded_sentence if c not in filtered_values]\n",
        "    return \" \".join(components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkPQWDq9W0jj"
      },
      "outputs": [],
      "source": [
        "for i in tqdm(np.random.choice(len(data), 10)):\n",
        "    item = data.iloc[i]\n",
        "    translated = decode_sequence_ta(loaded_model_ta, item[\"english\"])\n",
        "    print(\"English:\", item[\"english\"])\n",
        "    print(\"tamil:\", item[\"tamil\"].replace(\"[start] \", \"\").replace(\" [end]\", \"\"))\n",
        "    print(\"Translated:\", translated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnbYTif5peik"
      },
      "outputs": [],
      "source": [
        "data_hi = pd.read_csv(\"/content/hindi - data.csv\")\n",
        "data_hi.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJ-gOZYIplgJ"
      },
      "outputs": [],
      "source": [
        "data_hi[\"hindi\"] = data_hi[\"hindi\"].apply(lambda item: f\"{config.start_token} \" + item + f\" {config.end_token}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyomYHvgpoJH"
      },
      "outputs": [],
      "source": [
        "def hindi_standardize(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\"%re.escape(strip_chars), \"\")\n",
        "english_vectorization = TextVectorization(\n",
        "    max_tokens=config.vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=config.sequence_length,\n",
        ")\n",
        "hindi_vectorization = TextVectorization(\n",
        "    max_tokens=config.vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=config.sequence_length + 1,\n",
        "    standardize=hindi_standardize,\n",
        ")\n",
        "english_vectorization.adapt(list(data_hi[\"english\"]))\n",
        "hindi_vectorization.adapt(list(data_hi[\"hindi\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZFDcM-mpwga"
      },
      "outputs": [],
      "source": [
        "def preprocess(english, hindi):\n",
        "    english = english_vectorization(english)\n",
        "    hindi = hindi_vectorization(hindi)\n",
        "    return ({\"encoder_inputs\": english, \"decoder_inputs\": hindi[:, :-1]}, hindi[:, 1:])\n",
        "def make_dataset(df, batch_size, mode):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(df[\"english\"]), list(df[\"hindi\"])))\n",
        "    if mode == \"train\":\n",
        "       dataset = dataset.shuffle(batch_size * 4)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(preprocess)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE).cache()\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPmXecLtpxt-"
      },
      "outputs": [],
      "source": [
        "train, valid = train_test_split(data_hi, test_size=config.validation_split)\n",
        "train.shape, valid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPgqgcY7p0BG"
      },
      "outputs": [],
      "source": [
        "train_ds = make_dataset(train, batch_size=config.batch_size, mode=\"train\")\n",
        "valid_ds = make_dataset(valid, batch_size=config.batch_size, mode=\"valid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK7MF_A2p2r2"
      },
      "outputs": [],
      "source": [
        "def get_model(config):\n",
        "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "    x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "        config.vocab_size,\n",
        "        config.sequence_length,\n",
        "        config.embed_dim,\n",
        "        mask_zero=True\n",
        "    )(encoder_inputs)\n",
        "    encoder_outputs = keras_nlp.layers.TransformerEncoder(intermediate_dim=config.embed_dim, num_heads=config.num_heads)(x)\n",
        "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "    encoded_seq_inputs = keras.Input(shape=(None, config.embed_dim), name=\"decoder_state_inputs\")\n",
        "    x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "        config.vocab_size,\n",
        "        config.sequence_length,\n",
        "        config.embed_dim,\n",
        "        mask_zero=True\n",
        "    )(decoder_inputs)\n",
        "    x = keras_nlp.layers.TransformerDecoder(config.latent_dim, config.num_heads)(x, encoded_seq_inputs)\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    decoder_outputs = layers.Dense(config.vocab_size, activation=\"softmax\")(x)\n",
        "    decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "    transformer = keras.Model(\n",
        "        [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        "    )\n",
        "    transformer.compile(\n",
        "        \"adam\",\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\n",
        "            \"accuracy\"\n",
        "        ]\n",
        "    )\n",
        "    return transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtxwQN84qDk2"
      },
      "outputs": [],
      "source": [
        "model_hi = get_model(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDx9Alq9qIMU"
      },
      "outputs": [],
      "source": [
        "checkpoints = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"model_hi.tf\",\n",
        "    monitor=\"val_accuracy\",\n",
        "    mode=\"min\",\n",
        "    save_best_only=True\n",
        ")\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=10,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    restore_best_weights=True\n",
        ")\n",
        "history = model_hi.fit(train_ds, epochs=config.epochs, validation_data=valid_ds, callbacks=[checkpoints, early_stop])\n",
        "accuracy = model_hi.evaluate(train_ds, return_dict=True)['accuracy']\n",
        "print(f'Accuracy of Hindi: {accuracy*100:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28SzFXrgqUvs"
      },
      "outputs": [],
      "source": [
        "loaded_model_hi = tf.keras.models.load_model(\"model_hi.tf\", custom_objects={\n",
        "    \"TokenAndPositionEmbedding\": keras_nlp.layers.TokenAndPositionEmbedding,\n",
        "    \"TransformerEncoder\": keras_nlp.layers.TransformerEncoder,\n",
        "    \"TransformerDecoder\": keras_nlp.layers.TransformerDecoder\n",
        "})\n",
        "loaded_model_hi.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3Y0Iz-zqqdo"
      },
      "outputs": [],
      "source": [
        "hindi_vocab = hindi_vectorization.get_vocabulary()\n",
        "hindi_index_lookup = dict(zip(range(len(hindi_vocab)), hindi_vocab))\n",
        "start_index = hindi_vocab.index(config.start_token)\n",
        "end_index = hindi_vocab.index(config.end_token)\n",
        "unk_index = hindi_vocab.index(\"[UNK]\")\n",
        "def decode_sequence_hi(model_hi, input_sentence, filtered_values = [start_index, end_index, unk_index]):\n",
        "    tokenized_input_sentence = english_vectorization([input_sentence])\n",
        "    decoded_sentence = [start_index] + [0] * (config.sequence_length)\n",
        "    for i in range(config.sequence_length):\n",
        "        decoded_sentence_constant = tf.constant([decoded_sentence[:config.sequence_length]])\n",
        "        predictions = model_hi([tokenized_input_sentence, decoded_sentence_constant])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        decoded_sentence[i + 1] = sampled_token_index\n",
        "        if sampled_token_index == end_index:\n",
        "            break\n",
        "    components = [hindi_index_lookup[c] for c in decoded_sentence if c not in filtered_values]\n",
        "    return \" \".join(components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj7AUoUuquVs"
      },
      "outputs": [],
      "source": [
        "for i in tqdm(np.random.choice(len(data_hi), 10)):\n",
        "    item = data_hi.iloc[i]\n",
        "    translated = decode_sequence_hi(loaded_model_hi, item[\"english\"])\n",
        "    print(\"English:\", item[\"english\"])\n",
        "    print(\"hindi:\", item[\"hindi\"].replace(\"[start] \", \"\").replace(\" [end]\", \"\"))\n",
        "    print(\"Translated:\", translated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ype_KD51udU"
      },
      "outputs": [],
      "source": [
        "data_ml = pd.read_csv(\"/content/Malayalam - data.csv\")\n",
        "data_ml.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF7W4Gmr4ajQ"
      },
      "outputs": [],
      "source": [
        "data_ml[\"malayalam\"] = data_ml[\"malayalam\"].apply(lambda item: f\"{config.start_token} \" + item + f\" {config.end_token}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wINeKuxZ4duF"
      },
      "outputs": [],
      "source": [
        "strip_chars = string.punctuation + \"Â¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "def malayalam_standardize(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\"%re.escape(strip_chars), \"\")\n",
        "english_vectorization = TextVectorization(\n",
        "    max_tokens=config.vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=config.sequence_length,\n",
        ")\n",
        "malayalam_vectorization = TextVectorization(\n",
        "    max_tokens=config.vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=config.sequence_length + 1,\n",
        "    standardize=malayalam_standardize,\n",
        ")\n",
        "english_vectorization.adapt(list(data_ml[\"english\"]))\n",
        "malayalam_vectorization.adapt(list(data_ml[\"malayalam\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_otjRxQ4hOx"
      },
      "outputs": [],
      "source": [
        "def preprocess(english, malayalam):\n",
        "    english = english_vectorization(english)\n",
        "    malayalam = malayalam_vectorization(malayalam)\n",
        "    return ({\"encoder_inputs\": english, \"decoder_inputs\": malayalam[:, :-1]}, malayalam[:, 1:])\n",
        "def make_dataset(df, batch_size, mode):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(df[\"english\"]), list(df[\"malayalam\"])))\n",
        "    if mode == \"train\":\n",
        "       dataset = dataset.shuffle(batch_size * 4)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(preprocess)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE).cache()\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx4vdAWN4nbP"
      },
      "outputs": [],
      "source": [
        "train, valid = train_test_split(data_ml, test_size=config.validation_split)\n",
        "train.shape, valid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDmtNB224p39"
      },
      "outputs": [],
      "source": [
        "train_ds = make_dataset(train, batch_size=config.batch_size, mode=\"train\")\n",
        "valid_ds = make_dataset(valid, batch_size=config.batch_size, mode=\"valid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hesVtFgV4seC"
      },
      "outputs": [],
      "source": [
        "def get_model_ml(config):\n",
        "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "    x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "        config.vocab_size,\n",
        "        config.sequence_length,\n",
        "        config.embed_dim,\n",
        "        mask_zero=True\n",
        "    )(encoder_inputs)\n",
        "    encoder_outputs = keras_nlp.layers.TransformerEncoder(intermediate_dim=config.embed_dim, num_heads=config.num_heads)(x)\n",
        "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "    encoded_seq_inputs = keras.Input(shape=(None, config.embed_dim), name=\"decoder_state_inputs\")\n",
        "    x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "        config.vocab_size,\n",
        "        config.sequence_length,\n",
        "        config.embed_dim,\n",
        "        mask_zero=True\n",
        "    )(decoder_inputs)\n",
        "    x = keras_nlp.layers.TransformerDecoder(config.latent_dim, config.num_heads)(x, encoded_seq_inputs)\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    decoder_outputs = layers.Dense(config.vocab_size, activation=\"softmax\")(x)\n",
        "    decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "    transformer = keras.Model(\n",
        "        [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        "    )\n",
        "    transformer.compile(\n",
        "        \"adam\",\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\n",
        "            \"accuracy\"\n",
        "        ]\n",
        "    )\n",
        "    return transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6eLS4QW4xef"
      },
      "outputs": [],
      "source": [
        "model_ml = get_model_ml(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ2LX1JI41IA"
      },
      "outputs": [],
      "source": [
        "checkpoints = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"model_ml.tf\",\n",
        "    monitor=\"val_accuracy\",\n",
        "    mode=\"min\",\n",
        "    save_best_only=True\n",
        ")\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=10,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    restore_best_weights=True\n",
        ")\n",
        "history = model_ml.fit(train_ds, epochs=config.epochs, validation_data=valid_ds, callbacks=[checkpoints, early_stop])\n",
        "accuracy = model_ml.evaluate(train_ds, return_dict=True)['accuracy']\n",
        "print(f'Accuracy Of Malayalam: {accuracy*100:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBn4LnmQ5PHH"
      },
      "outputs": [],
      "source": [
        "loaded_model_ml = tf.keras.models.load_model(\"model_ml.tf\", custom_objects={\n",
        "    \"TokenAndPositionEmbedding\": keras_nlp.layers.TokenAndPositionEmbedding,\n",
        "    \"TransformerEncoder\": keras_nlp.layers.TransformerEncoder,\n",
        "    \"TransformerDecoder\": keras_nlp.layers.TransformerDecoder\n",
        "})\n",
        "loaded_model_ml.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s4mcWW25Tge"
      },
      "outputs": [],
      "source": [
        "malayalam_vocab = malayalam_vectorization.get_vocabulary()\n",
        "malayalam_index_lookup = dict(zip(range(len(malayalam_vocab)), malayalam_vocab))\n",
        "start_index = malayalam_vocab.index(config.start_token)\n",
        "end_index = malayalam_vocab.index(config.end_token)\n",
        "unk_index = malayalam_vocab.index(\"[UNK]\")\n",
        "def decode_sequence_ml(model_ml, input_sentence, filtered_values = [start_index, end_index, unk_index]):\n",
        "    tokenized_input_sentence = english_vectorization([input_sentence])\n",
        "    decoded_sentence = [start_index] + [0] * (config.sequence_length)\n",
        "    for i in range(config.sequence_length):\n",
        "        decoded_sentence_constant = tf.constant([decoded_sentence[:config.sequence_length]])\n",
        "        predictions = model_ml([tokenized_input_sentence, decoded_sentence_constant])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        decoded_sentence[i + 1] = sampled_token_index\n",
        "        if sampled_token_index == end_index:\n",
        "            break\n",
        "    components = [malayalam_index_lookup[c] for c in decoded_sentence if c not in filtered_values]\n",
        "    return \" \".join(components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ak9sUfuM5Uae"
      },
      "outputs": [],
      "source": [
        "for i in tqdm(np.random.choice(len(data_ml), 10)):\n",
        "    item = data_ml.iloc[i]\n",
        "    translated = decode_sequence_ml(loaded_model_ml, item[\"english\"])\n",
        "    print(\"English:\", item[\"english\"])\n",
        "    print(\"malayalam:\", item[\"malayalam\"].replace(\"[start] \", \"\").replace(\" [end]\", \"\"))\n",
        "    print(\"Translated:\", translated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YF7j9W7Z7AFD"
      },
      "outputs": [],
      "source": [
        "data_te = pd.read_csv(\"/content/telugu - data.csv\")\n",
        "data_te.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBsVDSid7vi4"
      },
      "outputs": [],
      "source": [
        "data_te[\"telugu\"] = data_te[\"telugu\"].apply(lambda item: f\"{config.start_token} \" + item + f\" {config.end_token}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjCEEuvY7yce"
      },
      "outputs": [],
      "source": [
        "strip_chars = string.punctuation + \"Â¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "def telugu_standardize(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\"%re.escape(strip_chars), \"\")\n",
        "english_vectorization = TextVectorization(\n",
        "    max_tokens=config.vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=config.sequence_length,\n",
        ")\n",
        "telugu_vectorization = TextVectorization(\n",
        "    max_tokens=config.vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=config.sequence_length + 1,\n",
        "    standardize=telugu_standardize,\n",
        ")\n",
        "english_vectorization.adapt(list(data_te[\"english\"]))\n",
        "telugu_vectorization.adapt(list(data_te[\"telugu\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfzPX8z273Pb"
      },
      "outputs": [],
      "source": [
        "def preprocess(english, telugu):\n",
        "    english = english_vectorization(english)\n",
        "    telugu = telugu_vectorization(telugu)\n",
        "    return ({\"encoder_inputs\": english, \"decoder_inputs\": telugu[:, :-1]}, telugu[:, 1:])\n",
        "def make_dataset(df, batch_size, mode):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(df[\"english\"]), list(df[\"telugu\"])))\n",
        "    if mode == \"train\":\n",
        "       dataset = dataset.shuffle(batch_size * 4)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(preprocess)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE).cache()\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZr79LBV76E3"
      },
      "outputs": [],
      "source": [
        "train, valid = train_test_split(data_te, test_size=config.validation_split)\n",
        "train.shape, valid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7U73Xh978dj"
      },
      "outputs": [],
      "source": [
        "train_ds = make_dataset(train, batch_size=config.batch_size, mode=\"train\")\n",
        "valid_ds = make_dataset(valid, batch_size=config.batch_size, mode=\"valid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CC324D107_pE"
      },
      "outputs": [],
      "source": [
        "def get_model_te(config):\n",
        "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "    x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "        config.vocab_size,\n",
        "        config.sequence_length,\n",
        "        config.embed_dim,\n",
        "        mask_zero=True\n",
        "    )(encoder_inputs)\n",
        "    encoder_outputs = keras_nlp.layers.TransformerEncoder(intermediate_dim=config.embed_dim, num_heads=config.num_heads)(x)\n",
        "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "    encoded_seq_inputs = keras.Input(shape=(None, config.embed_dim), name=\"decoder_state_inputs\")\n",
        "    x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "        config.vocab_size,\n",
        "        config.sequence_length,\n",
        "        config.embed_dim,\n",
        "        mask_zero=True\n",
        "    )(decoder_inputs)\n",
        "    x = keras_nlp.layers.TransformerDecoder(config.latent_dim, config.num_heads)(x, encoded_seq_inputs)\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    decoder_outputs = layers.Dense(config.vocab_size, activation=\"softmax\")(x)\n",
        "    decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "    transformer = keras.Model(\n",
        "        [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        "    )\n",
        "    transformer.compile(\n",
        "        \"adam\",\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\n",
        "            \"accuracy\"\n",
        "        ]\n",
        "    )\n",
        "    return transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ml9r6nH48DiZ"
      },
      "outputs": [],
      "source": [
        "model_te = get_model_te(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeMWJyOp8H9H"
      },
      "outputs": [],
      "source": [
        "checkpoints = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"model_te.tf\",\n",
        "    monitor=\"val_accuracy\",\n",
        "    mode=\"min\",\n",
        "    save_best_only=True\n",
        ")\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=10,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    restore_best_weights=True\n",
        ")\n",
        "history = model_te.fit(train_ds, epochs=config.epochs, validation_data=valid_ds, callbacks=[checkpoints, early_stop])\n",
        "accuracy = model_te.evaluate(train_ds, return_dict=True)['accuracy']\n",
        "print(f'Accuracy of Telugu: {accuracy*100:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2fUkPC38J89"
      },
      "outputs": [],
      "source": [
        "loaded_model_te = tf.keras.models.load_model(\"model_te.tf\", custom_objects={\n",
        "    \"TokenAndPositionEmbedding\": keras_nlp.layers.TokenAndPositionEmbedding,\n",
        "    \"TransformerEncoder\": keras_nlp.layers.TransformerEncoder,\n",
        "    \"TransformerDecoder\": keras_nlp.layers.TransformerDecoder\n",
        "})\n",
        "loaded_model_te.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcRYOaxa8MNv"
      },
      "outputs": [],
      "source": [
        "telugu_vocab = telugu_vectorization.get_vocabulary()\n",
        "telugu_index_lookup = dict(zip(range(len(telugu_vocab)), telugu_vocab))\n",
        "start_index = telugu_vocab.index(config.start_token)\n",
        "end_index = telugu_vocab.index(config.end_token)\n",
        "unk_index = telugu_vocab.index(\"[UNK]\")\n",
        "def decode_sequence_te(model_te, input_sentence, filtered_values = [start_index, end_index, unk_index]):\n",
        "    tokenized_input_sentence = english_vectorization([input_sentence])\n",
        "    decoded_sentence = [start_index] + [0] * (config.sequence_length)\n",
        "    for i in range(config.sequence_length):\n",
        "        decoded_sentence_constant = tf.constant([decoded_sentence[:config.sequence_length]])\n",
        "        predictions = model_te([tokenized_input_sentence, decoded_sentence_constant])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        decoded_sentence[i + 1] = sampled_token_index\n",
        "        if sampled_token_index == end_index:\n",
        "            break\n",
        "    components = [telugu_index_lookup[c] for c in decoded_sentence if c not in filtered_values]\n",
        "    return \" \".join(components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgPPrdth8OC6"
      },
      "outputs": [],
      "source": [
        "for i in tqdm(np.random.choice(len(data_te), 10)):\n",
        "    item = data_te.iloc[i]\n",
        "    translated = decode_sequence_te(loaded_model_te, item[\"english\"])\n",
        "    print(\"English:\", item[\"english\"])\n",
        "    print(\"telugu:\", item[\"telugu\"].replace(\"[start] \", \"\").replace(\" [end]\", \"\"))\n",
        "    print(\"Translated:\", translated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQk8COnd8j9B"
      },
      "source": [
        "***TAMIL***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIRzmewtqZnw"
      },
      "outputs": [],
      "source": [
        "def translate_user_input(input_sentence):\n",
        "    translated = decode_sequence_ta(loaded_model_ta, input_sentence)\n",
        "\n",
        "    print(\"Input English:\", input_sentence)\n",
        "    print(\"Translated Tamil:\", translated)\n",
        "\n",
        "# Example usage:\n",
        "user_input_sentence = \"Where is Tom?\"\n",
        "translate_user_input(user_input_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BTyo9Sc1xQu"
      },
      "source": [
        "***HINDI***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHM2MGfCvIWM"
      },
      "outputs": [],
      "source": [
        "def translate_user_input(input_sentence):\n",
        "    translated = decode_sequence_hi(loaded_model_hi, input_sentence)\n",
        "\n",
        "    print(\"Input English:\", input_sentence)\n",
        "    print(\"Translated hindi:\", translated)\n",
        "\n",
        "# Example usage:\n",
        "user_input_sentence = \"Tom is happy\"\n",
        "translate_user_input(user_input_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYS8Dvn117Zj"
      },
      "source": [
        "***MALAYALAM***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpU9Rryg5b7E"
      },
      "outputs": [],
      "source": [
        "def translate_user_input(input_sentence):\n",
        "    translated = decode_sequence_ml(loaded_model_ml, input_sentence)\n",
        "\n",
        "    print(\"Input English:\", input_sentence)\n",
        "    print(\"Translated malayalam:\", translated)\n",
        "\n",
        "# Example usage:\n",
        "user_input_sentence = \"We know her\"\n",
        "translate_user_input(user_input_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cyAMp6Y67SM"
      },
      "source": [
        "***TELUGU***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCIHH9sR8QC-"
      },
      "outputs": [],
      "source": [
        "def translate_user_input(input_sentence):\n",
        "    translated = decode_sequence_te(loaded_model_te, input_sentence)\n",
        "\n",
        "    print(\"Input English:\", input_sentence)\n",
        "    print(\"Translated telugu:\", translated)\n",
        "\n",
        "# Example usage:\n",
        "user_input_sentence = \"Tom is happy\"\n",
        "translate_user_input(user_input_sentence)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}